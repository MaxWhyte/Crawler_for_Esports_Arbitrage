{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawler for Esports Arbitrage\n",
    "\n",
    "This notebook crawls the bookmakers Orbit, BuffBet, 1xBit, Pinnacle/PS3838, SBObet and BetinAsia for odds of upcoming esports and soccer games and finds arbitrage opportunities in two-way matches like handicap bets or matches where there are only winners and losers but no draws.\n",
    "\n",
    "Tech used: this notebook relies heavily on Selenium for manipulating browser behaviour and BeautifulSoup/requests for scraping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Package import\n",
    "from selenium import webdriver\n",
    "import webbrowser\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import clear_output\n",
    "import json\n",
    "\n",
    "#for MacOS\n",
    "#from webdriver_manager.firefox import GeckoDriverManager "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the browser webdriver\n",
    "\n",
    "profile = webdriver.FirefoxProfile(\"C:\\\\Users\\\\ULTRA\\\\AppData\\\\Roaming\\\\Mozilla\\\\Firefox\\\\Profiles\\\\ffprofile.selenium\")\n",
    "\n",
    "profile.set_preference(\"dom.webdriver.enabled\", False)\n",
    "profile.set_preference('useAutomationExtension', False)\n",
    "profile.update_preferences()\n",
    "desired = DesiredCapabilities.FIREFOX\n",
    "\n",
    "#Windows:\n",
    "driver = webdriver.Firefox(executable_path=\"C:\\\\Users\\\\ULTRA\\\\Downloads\\\\tools\\\\geckodriver.exe\", firefox_profile=profile, desired_capabilities=desired)\n",
    "\n",
    "#MacOS:\n",
    "#driver = webdriver.Firefox(executable_path=GeckoDriverManager().install()) #, firefox_profile=profile, desired_capabilities=desired)\n",
    "\n",
    "#Checks setting box to open links in new tab\n",
    "driver.get(\"about:preferences\")\n",
    "checked = driver.find_element_by_id(\"linkTargeting\")\n",
    "checked.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define websites to visit\n",
    "PB = 'https://en.pari-match.com/en/e-sports'\n",
    "BB = 'https://buff.bet/en/esports'\n",
    "LB = 'https://loot.bet/'\n",
    "one_X = 'https://1xbit.com/line/Esports/'\n",
    "orbit_X = 'https://www.orbitxch.com/customer/sport/27454571'\n",
    "PS = 'https://www.ps3838.com/en/euro/sports/e-sports'\n",
    "PS_soccer = 'https://www.ps3838.com/en/euro/sports/soccer/germany'\n",
    "SBO_soccer = 'https://www.sbobet.com/euro/football/germany'\n",
    "BL = 'https://black.betinasia.com/login?next=/trade'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open new tabs and open URL\n",
    "\n",
    "for site in [orbit_X, orbit_X, BB, one_X, PS, PS_soccer, SBO_soccer, BL]:\n",
    "    script = 'window.open(\"' + site +'\");'\n",
    "    print(script)\n",
    "    driver.execute_script(script)\n",
    "\n",
    "# If neccessary you can login now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to grab the data from websites\n",
    "\n",
    "def get_data_1x():\n",
    "    ###\n",
    "    ### GRAB DATA from 1xbit\n",
    "    ###\n",
    "    driver.switch_to.window(driver.window_handles[4])\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Scrape relevant info from soup\n",
    "    team_list = []\n",
    "    odds_list = []\n",
    "    time_list = []\n",
    "\n",
    "    containers = soup.findAll(\"div\", {\"class\": \"c-events__item_col\"})\n",
    "    for container in containers:\n",
    "        teams = [x.get_text().strip() for x in container.findAll(\n",
    "            \"span\", {\"class\": \"c-events__team\"}\n",
    "        )]\n",
    "        if len(teams) == 2:\n",
    "            team_list.append(teams)\n",
    "\n",
    "            odds = [x.get_text().strip() for x in container.findAll(\n",
    "                \"span\", {\"class\": \"c-bets__bet\"}\n",
    "            )]\n",
    "            odds = [i for i in odds if i != \"-\"] # removes dashes\n",
    "\n",
    "            if len(odds) >= 3: # Marking 1x2 bets as invalid with \"NaN\"\n",
    "                odds = [\"NaN\", \"NaN\"]\n",
    "\n",
    "            odds_list.append(float(odd) for odd in odds)\n",
    "\n",
    "            times = [x.get_text().strip() for x in container.findAll(\n",
    "                \"div\", {\"class\": \"c-events__time\"}\n",
    "            )]\n",
    "            time_list.append(times)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # Create DF\n",
    "    df_1X = pd.DataFrame(team_list, columns=['Player_1', 'Player_2'])\n",
    "    df_odds = pd.DataFrame(odds_list, columns=['Odds_1', 'Odds_2'])\n",
    "    df_times = pd.DataFrame(time_list, columns=['Time'])\n",
    "    df_1X = df_1X.join(df_odds).join(df_times)\n",
    "    df_1X[\"Bet_Type\"] = \"1/2\"\n",
    "\n",
    "    return df_1X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_lb():\n",
    "    ###\n",
    "    ###GRAB DATA from LootBet\n",
    "    ###\n",
    "    driver.switch_to.window(driver.window_handles[number])\n",
    "    \n",
    "    y = 0\n",
    "    for timer in range(0,20):\n",
    "        driver.execute_script(\"window.scrollTo(0, \"+str(y)+\")\")\n",
    "        y += 35\n",
    "        time.sleep(0.01)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Scrape relevant info from soup\n",
    "    team_list = []\n",
    "    odds_list = []\n",
    "    date_list = []\n",
    "    time_list = []\n",
    "\n",
    "    containers = soup.findAll(\"div\", {\"class\": \"itemNew hover-market\"})\n",
    "    for container in containers:\n",
    "        teams = [x.get_text().strip() for x in container.findAll(\n",
    "            \"span\", {\"class\": \"name\"}\n",
    "        )]\n",
    "        if len(teams) == 3:\n",
    "            del teams[1]\n",
    "\n",
    "        team_list.append(teams)\n",
    "\n",
    "        odds = [x.get_text().strip() for x in container.findAll(\n",
    "            \"span\", {\"class\": \"cof\"}\n",
    "        )]\n",
    "        if len(odds) == 3: # Marking 1x2 bets as invalid with \"NaN\"\n",
    "            odds = [\"NaN\", \"NaN\"]\n",
    "        elif len(odds) > 3:\n",
    "            odds = odds[0:2]\n",
    "        odds_list.append(float(odd) for odd in odds)\n",
    "\n",
    "        date = [x.get_text().strip() for x in container.findAll(\n",
    "            \"span\", {\"class\": \"date\"}\n",
    "        )]\n",
    "        date_list.append(date)\n",
    "\n",
    "        times = [x.get_text().strip() for x in container.findAll(\n",
    "            \"span\", {\"class\": \"time\"}\n",
    "        )]\n",
    "        time_list.append(times)\n",
    "\n",
    "    #C reate DF\n",
    "    df_LB = pd.DataFrame(team_list, columns=['Player_1', 'Player_2'])\n",
    "    df_odds = pd.DataFrame(odds_list, columns=['Odds_1', 'Odds_2'])\n",
    "    df_dates = pd.DataFrame(date_list, columns=[\"Date\"])\n",
    "    df_times = pd.DataFrame(time_list, columns=['Time'])\n",
    "    df_LB = df_LB.join(df_odds).join(df_dates).join(df_times)\n",
    "\n",
    "    df_LB[\"Time\"] = df_LB[\"Date\"] + \" \" + df_LB[\"Time\"]\n",
    "    df_LB = df_LB.drop(\"Date\", axis=1)\n",
    "\n",
    "    df_LB = df_LB[:70] # Limit to 70 matches\n",
    "    \n",
    "    return df_LB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_pb():\n",
    "    ###\n",
    "    ##GRAB DATA from Pari\n",
    "    ###\n",
    "    driver.switch_to.window(driver.window_handles[number])\n",
    "    \n",
    "    y = 0\n",
    "    for timer in range(0,50):\n",
    "        driver.execute_script(\"window.scrollTo(0, \"+str(y)+\")\")\n",
    "        y += 35\n",
    "        time.sleep(0.01)\n",
    "        \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Scrape relevant info from soup\n",
    "    team_list = []\n",
    "    odds_list = []\n",
    "    date_list = []\n",
    "    time_list = []\n",
    "\n",
    "    containers = soup.findAll(\"div\", {\"class\": \"_3i6j5pH655bYkz5944HSdq\"}) #\n",
    "    for container in containers:\n",
    "        teams = [x.get_text().strip() for x in container.findAll(\n",
    "            \"span\", {\"class\": \"_2-4kPKVpNrNoq_0Ylv6TFX\"}\n",
    "        )]\n",
    "        team_list.append(teams)\n",
    "\n",
    "        odds = [x.get_text().strip() for x in container.findAll(\n",
    "            \"span\", {\"class\": \"_1TBLfOVfJx5AZbnqsME6Y5 _2QYjpCvQKAOO3cGwQDvfax\"}\n",
    "        )]\n",
    "        if len(odds) == 0:\n",
    "            odds = [\"NaN\", \"NaN\"]\n",
    "        if len(odds) == 3: # Marking 1x2 bets as invalid with \"NaN\"\n",
    "            odds = [\"NaN\", \"NaN\"]\n",
    "        if len(odds) == 4:\n",
    "            odds = odds[0:2]\n",
    "        if len(odds) == 5: # Marking 1x2 bets as invalid with \"NaN\"\n",
    "            odds = [\"NaN\", \"NaN\"]\n",
    "        if odds == ['--', '--']:\n",
    "            odds = [\"NaN\", \"NaN\"]\n",
    "\n",
    "        try: \n",
    "            odds_list.append(float(odd) for odd in odds)\n",
    "        except:\n",
    "            odds_list.append([\"NaN\", \"NaN\"])\n",
    "\n",
    "        dates = [x.get_text().strip() for x in container.findAll(\n",
    "            \"div\", {\"class\": \"sn74bYiEiedAa1IAXv20h\"}\n",
    "        )]\n",
    "        dates = dates[0].split(\"/\")\n",
    "        date = dates[0].strip() + \" 2021\"\n",
    "        times = dates[1].strip()\n",
    "\n",
    "        date_list.append(date)\n",
    "        time_list.append(times)\n",
    "        \n",
    "    # Create DF\n",
    "    df_PB = pd.DataFrame(team_list, columns=['Player_1', 'Player_2'])\n",
    "    df_odds = pd.DataFrame(odds_list, columns=['Odds_1', 'Odds_2'])\n",
    "    df_dates = pd.DataFrame(date_list, columns=[\"Date\"])\n",
    "    df_times = pd.DataFrame(time_list, columns=['Time'])\n",
    "    df_PB = df_PB.join(df_odds).join(df_dates).join(df_times)\n",
    "\n",
    "    df_PB[\"Time\"] = df_PB[\"Date\"] + \" \" + df_PB[\"Time\"]\n",
    "    df_PB = df_PB.drop(\"Date\", axis=1)\n",
    "    \n",
    "    return df_PB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_bb():\n",
    "    ###\n",
    "    #GRAB DATA from Buff\n",
    "    ###\n",
    "    \n",
    "    # Alternative: when site is blocked by hcaptcha:\n",
    "    # Copy the html code from the website into a textfile \"txt.txt\" and save it\n",
    "    # The following 3 lines of code reads html code from the local textfile and\n",
    "    # stores it in \"soup\"\n",
    "    # If you do this, uncomment the following 3 lines and comment out the lines 16 and 17\n",
    "    \n",
    "    #f = open(\"txt.txt\", encoding=\"utf8\")\n",
    "    #data = f.read()\n",
    "    #soup = BeautifulSoup(data, 'html.parser')\n",
    "    \n",
    "    driver.switch_to.window(driver.window_handles[3])\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Scrape relevant info from soup\n",
    "    team_list = []\n",
    "    odds_list = []\n",
    "    date_list = []\n",
    "    time_list = []\n",
    "\n",
    "    containers = soup.findAll(\"div\", {\"class\": \"_38bJi\"}) #\n",
    "    for container in containers:\n",
    "        teams = [x.get_text().strip() for x in container.findAll(\n",
    "            \"a\", {\"class\": \"_3bf_k _1WIu4\"}\n",
    "        )]\n",
    "        if len(teams) == 3:\n",
    "            del teams[1]\n",
    "        team_list.append(teams)\n",
    "\n",
    "        odds = [x.get_text().strip() for x in container.findAll(\"div\", {\"class\": \"_1sT8o _2ALVH\"}\n",
    "        )]\n",
    "        if len(odds) == 0 or odds == ['--', '--'] or odds == ['', ''] or len(odds) == 3: # Marking 1x2 bets as invalid with \"NaN\"\n",
    "            odds = [\"NaN\", \"NaN\"]\n",
    "        elif len(odds) > 3:\n",
    "            odds = odds[0:2]\n",
    "        for odd in odds:\n",
    "            if odd == \"\":\n",
    "                odds = [\"NaN\", \"NaN\"]\n",
    "        try: \n",
    "            odds_list.append(float(odd.replace(\",\",\".\")) for odd in odds)\n",
    "        except:\n",
    "            odds_list.append([\"NaN\", \"NaN\"])\n",
    "\n",
    "        dates = [x.get_text().strip() for x in container.findAll(\n",
    "            \"div\", {\"class\": \"_1jkHU\"}\n",
    "        )]\n",
    "        if len(dates) == 0:\n",
    "            dates = [\"LIVE LIVE LIVE\"]\n",
    "        dates = dates[0].split(\" \")\n",
    "        date = str(dates[0]) + \" \" + str(dates[1]) + \" 2021\"\n",
    "        times = dates[2]\n",
    "        date_list.append(date)\n",
    "        time_list.append(times)\n",
    "        \n",
    "    # Create DF\n",
    "    df_BB = pd.DataFrame(team_list, columns=['Player_1', 'Player_2'])\n",
    "    df_odds = pd.DataFrame(odds_list, columns=['Odds_1', 'Odds_2'])\n",
    "    df_dates = pd.DataFrame(date_list, columns=[\"Date\"])\n",
    "    df_times = pd.DataFrame(time_list, columns=['Time'])\n",
    "    df_BB = df_BB.join(df_odds).join(df_dates).join(df_times)\n",
    "\n",
    "    df_BB[\"Time\"] = df_BB[\"Date\"] + \" \" + df_BB[\"Time\"]\n",
    "    df_BB[\"Bet_Type\"] = \"1/2\"\n",
    "    df_BB = df_BB.drop(\"Date\", axis=1)\n",
    "    \n",
    "    return df_BB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_ox_page(window):\n",
    "    ###\n",
    "    #GRAB DATA from Orbit/Betfair\n",
    "    ###\n",
    "    driver.switch_to.window(driver.window_handles[window])\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    team_list = []\n",
    "    odds_list = []\n",
    "    date_list = []\n",
    "    time_list = []\n",
    "    lays = []\n",
    "\n",
    "    containers = soup.findAll(\"div\", {\"class\": \"biab_table-wrapper\"}) #\n",
    "    for container in containers:\n",
    "        teams = [x for x in container.findAll(\n",
    "            \"div\", {\"class\": \"biab_market-title-team-names js-teams\"})]\n",
    "\n",
    "        for word in ['<div class=\"biab_market-title-team-names js-teams\">', '</div>', '[', ']']:\n",
    "            teams = str(teams).replace(word,'')\n",
    "            teams = str(teams).replace('<br/>',', ')\n",
    "        team_1 = teams.split(\",\")[0]\n",
    "        team_2 = teams.split(\",\")[1:][0].strip()\n",
    "        teams = [team_1, team_2]\n",
    "        team_list.append(teams)\n",
    "\n",
    "        odds = [x.get_text().strip() for x in container.findAll(\n",
    "            \"span\", {\"class\": \"js-odds biab_odds\"})]\n",
    "\n",
    "        if len(odds) == 4:\n",
    "            lay = []\n",
    "            lay.append(float(odds[1]))\n",
    "            lay.append(float(odds[3]))\n",
    "            lays.append(lay)\n",
    "            del odds[1]\n",
    "            del odds[2]\n",
    "        else:\n",
    "            lays.append([0,0])\n",
    "        \n",
    "        if len(odds) == 4:\n",
    "            \n",
    "            del odds[1]\n",
    "            del odds[2]\n",
    "        if len(odds) == 0:\n",
    "            odds = [\"NaN\", \"NaN\"]\n",
    "        if len(odds) == 3:\n",
    "            odds = [\"NaN\", \"NaN\"]\n",
    "        if len(odds) == 5:\n",
    "            odds = [\"NaN\", \"NaN\"]\n",
    "        if len(odds) == 6:\n",
    "            odds = [\"NaN\", \"NaN\"]\n",
    "        if odds == ['--', '--', '--', '--']:\n",
    "            odds = [\"NaN\", \"NaN\"]\n",
    "\n",
    "        try: \n",
    "            odds_list.append(float(odd.replace(\",\",\".\")) for odd in odds)\n",
    "        except:\n",
    "            odds_list.append([\"NaN\", \"NaN\"])\n",
    "\n",
    "        dates = [x.get_text().strip() for x in soup.findAll(\n",
    "            \"div\", {\"class\": \"biab_inplay-sport-item-title\"}\n",
    "            )]\n",
    "\n",
    "        try:\n",
    "            date = dates[0][4:] + \" 2021\"\n",
    "        except:\n",
    "            date = ['---']\n",
    "\n",
    "        date_list.append(date)\n",
    "\n",
    "        times = [x.get_text().strip() for x in container.findAll(\n",
    "            \"span\", {\"class\": \"biab_market-time\"}\n",
    "            )]\n",
    "        if times == ['In-Play'] or times == ['Starting soon']:\n",
    "            times = ['LIVE']\n",
    "        time_list.append(times)\n",
    "        \n",
    "    # Create DF\n",
    "    df_OX = pd.DataFrame(team_list, columns=['Player_1', 'Player_2'])\n",
    "    df_odds = pd.DataFrame(odds_list, columns=['Odds_1', 'Odds_2'])\n",
    "    df_lays = pd.DataFrame(lays, columns=[\"Lay_1\", \"Lay_2\"])\n",
    "    df_dates = pd.DataFrame(date_list, columns=[\"Date\"])\n",
    "    df_times = pd.DataFrame(time_list, columns=['Time'])\n",
    "    df_OX = df_OX.join(df_odds).join(df_lays).join(df_dates).join(df_times)\n",
    "\n",
    "    df_OX[\"Time\"] = df_OX[\"Date\"] + \" \" + df_OX[\"Time\"]\n",
    "    df_OX[\"Bet_Type\"] = \"1/2\"\n",
    "    df_OX = df_OX.drop(\"Date\", axis=1)\n",
    "    \n",
    "    \n",
    "    return df_OX\n",
    "\n",
    "def get_data_ox():\n",
    "    data_1 = get_data_ox_page(1)\n",
    "    data_2 = get_data_ox_page(2)\n",
    "    L = [data_1, data_2]\n",
    "    df_OX = pd.concat(L)\n",
    "    \n",
    "    return df_OX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_ps_page():\n",
    "    ###\n",
    "    ### Grab data from PS3838/Pinnacle\n",
    "    ###\n",
    "    driver.switch_to.window(driver.window_handles[5])\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    team_list = []\n",
    "    odds_list = []\n",
    "    date_list = []\n",
    "    time_list = []\n",
    "\n",
    "    containers = soup.findAll(\"tr\", {\"class\": \"status_O\"}) #\n",
    "\n",
    "    for container in containers:\n",
    "\n",
    "        # Get teams\n",
    "        teams = [x.get_text().strip().replace(u'\\u200e',\"\") for x in container.findAll(\n",
    "            \"span\", {\"class\": \"team_name onextwo\"})]\n",
    "        if len(teams) != 0:\n",
    "            team_list.append(teams)\n",
    "\n",
    "        # Get odds\n",
    "        odds = [x.get_text() for x in container.findAll(\"span\", {\"class\": \"o_right\"})]\n",
    "        \n",
    "        if len(teams) != 0:\n",
    "            try:\n",
    "                if odds != ['', '']:\n",
    "                    odds_list.append(float(odd) for odd in odds)\n",
    "                else:\n",
    "                    odds_list.append([\"Nan\", \"Nan\"])\n",
    "            except:\n",
    "                odds_list.append([\"Nan\", \"Nan\"])\n",
    "                \n",
    "        # Get date & time\n",
    "        date = [x.get_text() for x in container.findAll(\"span\", {\"class\": \"DateTime\"})]\n",
    "        \n",
    "        if date[0].count(' ') > 1:\n",
    "            date = [date[0]]\n",
    "            times = [\"LIVE\"]\n",
    "        else:\n",
    "            times = [date[0][-5:]]\n",
    "            date = [date[0][:6]]\n",
    "            \n",
    "        if len(teams) != 0:\n",
    "            date_list.append(date)\n",
    "            time_list.append(times)\n",
    "\n",
    "    # Create DF\n",
    "    df_PS = pd.DataFrame(team_list, columns=['Player_1', 'Player_2'])\n",
    "    df_odds = pd.DataFrame(odds_list, columns=['Odds_1', 'Odds_2'])\n",
    "    df_dates = pd.DataFrame(date_list, columns=[\"Date\"])\n",
    "    df_times = pd.DataFrame(time_list, columns=['Time'])\n",
    "    df_PS = df_PS.join(df_odds).join(df_dates).join(df_times)\n",
    "\n",
    "    df_PS[\"Time\"] = df_PS[\"Date\"] + \" \" + df_PS[\"Time\"]\n",
    "    df_PS[\"Bet_Type\"] = \"1/2\"\n",
    "    df_PS = df_PS.drop(\"Date\", axis=1)\n",
    "\n",
    "    return df_PS\n",
    "\n",
    "def get_data_ps():\n",
    "    data_1 = get_data_ps_page()\n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_css_selector('.dateMenutb > tbody:nth-child(1) > tr:nth-child(1) > td:nth-child(2)').click() \n",
    "    time.sleep(1)\n",
    "    data_2 = get_data_ps_page()\n",
    "    L = [data_1, data_2]\n",
    "    df_PS = pd.concat(L)\n",
    "    driver.find_element_by_css_selector('.dateMenutb > tbody:nth-child(1) > tr:nth-child(1) > td:nth-child(1)').click()\n",
    "    \n",
    "    return df_PS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_fl():\n",
    "    ###\n",
    "    ##GRAB DATA from Fairlay\n",
    "    ###\n",
    "    \n",
    "    # Grabbing data from Fairlay\n",
    "    r = requests.get('http://83.171.236.114:8080/free/markets/{\"Cat\":32,\"Descr\":\"Match\", \"_Period\":[1],\"_Type\":[0]}')\n",
    "    res = json.loads(r.content.decode('utf-8'))\n",
    "    \n",
    "    #Scrape relevant info from API request\n",
    "    team_list = []\n",
    "    odds_list = []\n",
    "    date_list = []\n",
    "    time_list = []\n",
    "    commission = 0\n",
    "    \n",
    "    for game in res:\n",
    "        if game[\"Descr\"] == \"Match\":\n",
    "            team_1 = game[\"Ru\"][0][\"Name\"] # Player_1\n",
    "            team_2 = game[\"Ru\"][1][\"Name\"] # Player_2\n",
    "            team_list.append([team_1, team_2])\n",
    "\n",
    "            odds = game[\"OrdBStr\"]\n",
    "            if len(odds) == 0 or odds == \"~\":\n",
    "                odds = 0\n",
    "                odds_list.append([odds, odds])\n",
    "            else:\n",
    "                odds = odds[:-1]\n",
    "                dic = json.loads(odds)\n",
    "                odd_back = float(dic[\"Bids\"][0][0])\n",
    "                odd_lay = float(dic[\"Asks\"][0][0])\n",
    "                odd_lay_eff = round(1 + ((1-commission)/(odd_lay- 1)), 3)\n",
    "                odds_list.append([odd_back, odd_lay_eff])\n",
    "\n",
    "            time_ = game[\"ClosD\"]\n",
    "            try:\n",
    "                time_ = datetime.fromisoformat(time_[:-1])\n",
    "                game_time = time_.strftime('%H:%M')\n",
    "                game_day = time_.strftime('%d-%m-%Y')\n",
    "            except:\n",
    "                time_ = \"NaN\"\n",
    "                game_time = \"NaN\"\n",
    "                game_day = \"NaN\"\n",
    "            date_list.append([game_day])\n",
    "            time_list.append([game_time])\n",
    "        \n",
    "    # Create DF\n",
    "    df_FL = pd.DataFrame(team_list, columns=['Player_1', 'Player_2'])\n",
    "    df_odds = pd.DataFrame(odds_list, columns=['Odds_1', 'Odds_2'])\n",
    "    df_dates = pd.DataFrame(date_list, columns=[\"Date\"])\n",
    "    df_times = pd.DataFrame(time_list, columns=['Time'])\n",
    "    df_FL = df_FL.join(df_odds).join(df_dates).join(df_times)\n",
    "\n",
    "    df_FL[\"Time\"] = df_FL[\"Date\"] + \" \" + df_FL[\"Time\"]\n",
    "    df_FL[\"Bet_Type\"] = \"1/2\"\n",
    "    df_FL = df_FL.drop(\"Date\", axis=1)\n",
    "    \n",
    "    return df_FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_fl_soccer():\n",
    "    ###\n",
    "    ##GRAB Soccer DATA from Fairlay\n",
    "    ###\n",
    "    \n",
    "    # Grabbing data from Fairlay\n",
    "    r = requests.get('http://83.171.236.114:8080/free/markets/%7B%22Cat%22:1,%22Descr%22:%22Spread%22,%20%22Comp%22:%20%22Germany%20-%20Bundesliga%202%22%7D')\n",
    "    res = json.loads(r.content.decode('utf-8'))\n",
    "    \n",
    "    # Scrape relevant info from API request\n",
    "    team_list = []\n",
    "    odds_list = []\n",
    "    date_list = []\n",
    "    time_list = []\n",
    "    bet_type_list = []\n",
    "    commission = 0\n",
    "    \n",
    "    for game in res:\n",
    "         if \"Spread\" in game[\"Descr\"]:\n",
    "            team_1 = game[\"Ru\"][0][\"Name\"] # Player_1\n",
    "            team_2 = game[\"Ru\"][1][\"Name\"] # Player_2\n",
    "            team_list.append([team_1, team_2])\n",
    "\n",
    "            odds = game[\"OrdBStr\"]\n",
    "            if len(odds) == 0 or odds == \"~\":\n",
    "                odds = 0\n",
    "                odds_list.append([odds, odds])\n",
    "            else:\n",
    "                odds = odds[:-1]\n",
    "                dic = json.loads(odds)\n",
    "                odd_back = float(dic[\"Bids\"][0][0])\n",
    "                odd_lay = float(dic[\"Asks\"][0][0])\n",
    "                odd_lay_eff = round(1 + ((1-commission)/(odd_lay- 1)), 3)\n",
    "                odds_list.append([odd_back, odd_lay_eff])\n",
    "\n",
    "            time_ = game[\"ClosD\"]\n",
    "            try:\n",
    "                time_ = datetime.fromisoformat(time_[:-1])\n",
    "                game_time = time_.strftime('%H:%M')\n",
    "                game_day = time_.strftime('%d-%m-%Y')\n",
    "            except:\n",
    "                time_ = \"NaN\"\n",
    "                game_time = \"NaN\"\n",
    "                game_day = \"NaN\"\n",
    "            date_list.append([game_day])\n",
    "            time_list.append([game_time])\n",
    "            \n",
    "            bet_type_list.append(game[\"Descr\"])\n",
    "        \n",
    "    # Create DF\n",
    "    df_FL = pd.DataFrame(team_list, columns=['Player_1', 'Player_2'])\n",
    "    df_odds = pd.DataFrame(odds_list, columns=['Odds_1', 'Odds_2'])\n",
    "    df_dates = pd.DataFrame(date_list, columns=[\"Date\"])\n",
    "    df_times = pd.DataFrame(time_list, columns=['Time'])\n",
    "    df_bet_types = pd.DataFrame(bet_type_list, columns=['Bet_Type'])\n",
    "    df_FL = df_FL.join(df_odds).join(df_dates).join(df_times).join(df_bet_types)\n",
    "    df_FL[\"Time\"] = df_FL[\"Date\"] + \" \" + df_FL[\"Time\"]\n",
    "    \n",
    "    for nr in [\"0.0\", \"0.25\", \"0.5\", \"0.75\", \"1.0\", \"1.25\", \"1.5\", \"1.75\", \"2.0\",\"2.5\", \"-0.0\", \"-0.25\", \"-0.5\", \"-0.75\", \"-1.0\", \"-1.25\", \"-1.5\", \"-1.75\", \"-2.0\", \"-2.5\"]:\n",
    "        bet = \"Match Spread for HomeTeam \"\n",
    "        string = bet + nr\n",
    "        df_FL= df_FL.replace(to_replace =string, \n",
    "                            value =\" AHM\")\n",
    "    for nr in [\"0.0\", \"0.25\", \"0.5\", \"0.75\", \"1.0\", \"1.25\", \"1.5\", \"1.75\", \"2.0\",\"2.5\", \"-0.0\", \"-0.25\", \"-0.5\", \"-0.75\", \"-1.0\", \"-1.25\", \"-1.5\", \"-1.75\", \"-2.0\", \"-2.5\"]:\n",
    "        bet = \"1st Half Spread for HomeTeam \"\n",
    "        string = bet + nr\n",
    "        df_FL= df_FL.replace(to_replace =string, \n",
    "                            value =\"AH1\")\n",
    "    df_FL[\"Player_1\"] = df_FL[\"Player_1\"] + df_FL[\"Bet_Type\"].str.lower()\n",
    "    df_FL[\"Player_2\"] = df_FL[\"Player_2\"] + df_FL[\"Bet_Type\"].str.lower()\n",
    "    df_FL = df_FL.drop(\"Date\", axis=1)\n",
    "    \n",
    "    return df_FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soccer_data_ps_page():\n",
    "    ###\n",
    "    #Grab Asian Handicap Match data for SOCCER from PS3838/Pinnacle\n",
    "    ###\n",
    "\n",
    "    driver.switch_to.window(driver.window_handles[6])\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    team_list = []\n",
    "    odds_list = []\n",
    "    date_list = []\n",
    "    time_list = []\n",
    "    handicap_list = []\n",
    "\n",
    "    for handicap_type in [\"HDP_0\", \"HDP_1\"]:\n",
    "        try:\n",
    "            bet_type = soup.find(\"div\", {\"id\": handicap_type})\n",
    "\n",
    "            for container in bet_type.findAll(\"tr\", {\"class\": \"status_O\"}):\n",
    "                # Get teams\n",
    "                teams = [x.get_text().strip().replace(u'\\u200e',\"\") for x in container.findAll(\"span\", {\"class\": \"team_name handicap\"})]\n",
    "                teams_handicap = [x.get_text().strip().replace(u'\\u200e',\"\") for x in container.findAll(\"span\", {\"class\": \"o_middle\"})]\n",
    "                teams = [\"\".join([teams[0], \" \", teams_handicap[0]]), \"\".join([teams[1], \" \", teams_handicap[1]])]\n",
    "                if len(teams) != 0:\n",
    "                    team_list.append(teams)\n",
    "\n",
    "                # Get odds\n",
    "                odds = [x.get_text() for x in container.findAll(\"span\", {\"class\": \"o_right\"})]\n",
    "\n",
    "                if len(teams) != 0:\n",
    "                    try:\n",
    "                        #print(odds)\n",
    "                        if odds != ['', '']:\n",
    "                            odds_list.append(float(odd) for odd in odds)\n",
    "                        else:\n",
    "                            odds_list.append([\"Nan\", \"Nan\"])\n",
    "                    except:\n",
    "                        odds_list.append([\"Nan\", \"Nan\"])\n",
    "\n",
    "\n",
    "                # Get date & time\n",
    "                date = [x.get_text() for x in container.findAll(\"span\", {\"class\": \"DateTime\"})]\n",
    "\n",
    "                if date[0].count(' ') > 1:\n",
    "                    date = [date[0]]\n",
    "                    times = [\"LIVE\"]\n",
    "                else:\n",
    "                    times = [date[0][-5:]]\n",
    "                    date = [date[0][:6]]\n",
    "\n",
    "                if len(teams) != 0:\n",
    "                    date_list.append(date)\n",
    "                    time_list.append(times)\n",
    "\n",
    "                # Get handicap type\n",
    "                handicap_list.append(handicap_type)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Create DF\n",
    "    df_PS = pd.DataFrame(team_list, columns=['Player_1', 'Player_2'])\n",
    "    df_odds = pd.DataFrame(odds_list, columns=['Odds_1', 'Odds_2'])\n",
    "    df_dates = pd.DataFrame(date_list, columns=[\"Date\"])\n",
    "    df_times = pd.DataFrame(time_list, columns=['Time'])\n",
    "    df_PS = df_PS.join(df_odds).join(df_dates).join(df_times)\n",
    "    df_handicap = pd.DataFrame(handicap_list, columns=['Bet_Type'])\n",
    "\n",
    "    df_PS[\"Time\"] = df_PS[\"Date\"] + \" \" + df_PS[\"Time\"]\n",
    "    df_PS[\"Bet_Type\"] = df_handicap\n",
    "    df_PS= df_PS.replace(to_replace =\"HDP_0\", \n",
    "                            value =\" AHM\")\n",
    "    df_PS = df_PS.replace(to_replace =\"HDP_1\", \n",
    "                            value =\" AH1\")\n",
    "    \n",
    "    df_PS[\"Player_1\"] = df_PS[\"Player_1\"] + df_PS[\"Bet_Type\"]\n",
    "    df_PS[\"Player_2\"] = df_PS[\"Player_2\"] + df_PS[\"Bet_Type\"]\n",
    "    \n",
    "    df_PS = df_PS.drop([\"Date\"], axis=1)\n",
    "\n",
    "    return df_PS\n",
    "\n",
    "def get_soccer_data_ps():\n",
    "    data_1 = get_soccer_data_ps_page()\n",
    "    time.sleep(2)\n",
    "    driver.find_element_by_css_selector('.dateMenutb > tbody:nth-child(1) > tr:nth-child(1) > td:nth-child(2)').click() \n",
    "    time.sleep(2)\n",
    "    data_2 = get_soccer_data_ps_page()\n",
    "    L = [data_1, data_2]\n",
    "    df_PS_soccer = pd.concat(L)\n",
    "    driver.find_element_by_css_selector('.dateMenutb > tbody:nth-child(1) > tr:nth-child(1) > td:nth-child(1)').click()\n",
    "    \n",
    "    return df_PS_soccer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_sb_page():  \n",
    "    ###\n",
    "    #Grab data from SBObet\n",
    "    ###\n",
    "    driver.switch_to.window(driver.window_handles[7])\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    team_list = []\n",
    "    odds_list = []\n",
    "    time_list = []\n",
    "    date_list = []\n",
    "    handicap_list = []\n",
    "    titles_list = []\n",
    "    rows = []\n",
    "    game = []\n",
    "\n",
    "    markets = soup.findAll(\"div\", {\"class\": \"MarketT\"})\n",
    "\n",
    "    for container in markets:\n",
    "        handicap_name = container.span.text\n",
    "        handicap_list.append(handicap_name)\n",
    "\n",
    "        for container in container.findAll(\"tr\"):\n",
    "            teams = []\n",
    "            for team in container.findAll(\"span\", {\"class\": \"OddsL\"}):\n",
    "                teams.append(team.text)\n",
    "\n",
    "                handicaps = []\n",
    "                for handicap in container.findAll(\"span\", {\"class\": \"OddsM\"}):\n",
    "                    handicaps.append(handicap.get_text())\n",
    "\n",
    "                odds = []\n",
    "                for odd in container.findAll(\"span\", {\"class\": \"OddsR\"}):\n",
    "                    odds.append(float(odd.get_text()))\n",
    "                odds_list.append(odds)\n",
    "\n",
    "                dates = []\n",
    "                for time_block in container.findAll(\"div\", {\"class\": \"DateTimeDiv\"}):\n",
    "                    game_ = []\n",
    "                    t = (time_block.get_text())[:-5]\n",
    "                    time_list.append(t)\n",
    "                    d = (time_block.get_text())[-5:]\n",
    "                    date_list.append(d)\n",
    "\n",
    "            rows.append([handicap_name, teams[0], teams[1], handicaps[0], handicaps[1], odds[0], odds[1], d, t])\n",
    "\n",
    "    # Create DF\n",
    "    df_SB = pd.DataFrame(rows, columns=[\"Bet_Type\", \"Player_1\", \"Player_2\", \"H_1\", \"H_2\", \"Odds_1\", \"Odds_2\", \"Time\", \"Date\"])\n",
    "    df_SB= df_SB.replace(to_replace =\"Asian Handicap\", value =\"AHM\")\n",
    "    df_SB = df_SB.replace(to_replace =\"First Half Asian Handicap\", value =\"AH1\")\n",
    "    df_SB[\"Player_1\"] = df_SB[\"Player_1\"] + \" \" + df_SB[\"H_1\"] + \" \" + df_SB[\"Bet_Type\"]\n",
    "    df_SB[\"Player_2\"] = df_SB[\"Player_2\"] + \" \" + df_SB[\"H_2\"] + \" \" + df_SB[\"Bet_Type\"]\n",
    "    df_SB[\"Time\"] = df_SB[\"Time\"] + \" \" + df_SB[\"Date\"]\n",
    "    df_SB = df_SB.drop([\"H_1\", \"H_2\", \"Date\"], axis=1)\n",
    "\n",
    "    return df_SB\n",
    "\n",
    "def get_data_sb():\n",
    "    data_1 = get_data_sb_page()\n",
    "    time.sleep(2)\n",
    "    driver.find_element_by_css_selector('#bu\\:od\\:go\\:dt\\:3').click() \n",
    "    time.sleep(5)\n",
    "    driver.find_element_by_css_selector('#bu\\:od\\:go\\:mt\\:2').click()\n",
    "    time.sleep(3)\n",
    "    data_2 = get_data_sb_page()\n",
    "    L = [data_1, data_2]\n",
    "    df_SB = pd.concat(L)\n",
    "    driver.find_element_by_css_selector('#bu\\:od\\:go\\:dt\\:2').click()\n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_css_selector('#bu\\:od\\:go\\:mt\\:2').click()\n",
    "    \n",
    "    return df_SB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_BL():\n",
    "    \n",
    "    driver.switch_to.window(driver.window_handles[8])\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    handicap_list = []\n",
    "    team_list = []\n",
    "    odds_list = []\n",
    "    ah_odds_list = []\n",
    "    date_list = []\n",
    "    time_list = []\n",
    "    competition_list = []\n",
    "    rows = []\n",
    "\n",
    "    markets = soup.findAll(\"div\", {\"class\": \"market\"})\n",
    "    \n",
    "    # handicap data\n",
    "    for container in markets:\n",
    "        for day in container.findAll(\"span\", {\"class\": \"name\"}):\n",
    "            day = day.text\n",
    "            date_list.append(day)\n",
    "        for competition in container.findAll(\"div\", {\"class\": \"name\"}):\n",
    "            competition = competition.text\n",
    "            competition_list.append(competition)\n",
    "        for time_ in container.findAll(\"div\", {\"class\": \"time\"}):\n",
    "            time_ = time_.text\n",
    "            time_list.append(time_)\n",
    "        for match in container.findAll(\"span\", {\"class\": \"info\"}):\n",
    "            for home in match.findAll(\"div\", {\"class\": \"home\"}):\n",
    "                home = home.text\n",
    "                home = home.split(\" - \")[1]\n",
    "            for away in match.findAll(\"div\", {\"class\": \"away\"}):\n",
    "                away = away.text\n",
    "                away = away.split(\" - \")[1]\n",
    "            team_list.append([home, away])\n",
    "\n",
    "        for odd_panel in container.findAll(\"div\", {\"class\": \"offer-group timeWin_tp_all_ml default\"}):\n",
    "            if len(odd_panel.text) > 0:\n",
    "                for odd_1 in odd_panel.findAll(\"span\", {\"class\": \"offer-price\"})[:1]:\n",
    "                    odd_1 = odd_1.get_text()\n",
    "                    if odd_1 == \"⇆\":\n",
    "                        odd_1 = \"NaN\"\n",
    "                    else:\n",
    "                        odd_1 = float(odd_1)\n",
    "                    \n",
    "                for odd_2 in odd_panel.findAll(\"span\", {\"class\": \"offer-price\"})[1:]:\n",
    "                    odd_2 = odd_2.get_text()\n",
    "                    if odd_2 == \"⇆\":\n",
    "                        odd_2 = \"NaN\"\n",
    "                    else:\n",
    "                        odd_2 = float(odd_2)\n",
    "                odds_list.append([(odd_1), (odd_2)])\n",
    "            else:\n",
    "                odds_list.append([\"NaN\", \"NaN\"])\n",
    "\n",
    "    df_BL = pd.DataFrame(time_list, columns=['Time'])\n",
    "    df_players = pd.DataFrame(team_list, columns=['Player_1', \"Player_2\"])\n",
    "    df_odds = pd.DataFrame(odds_list, columns=['Odds_1', \"Odds_2\"])\n",
    "    df_BL = df_BL.join(df_players).join(df_odds)\n",
    "    df_BL[\"Bet_Type\"] = \"1/2\"\n",
    "    df_BL[[\"Odds_1\", \"Odds_2\"]] = df_BL[[\"Odds_1\", \"Odds_2\"]].apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    return df_BL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv():\n",
    "    ###\n",
    "    #Saving single DFs as single CSV\n",
    "    ###\n",
    "    \n",
    "    df_1X.to_csv(\"df_1X.csv\")\n",
    "    df_BB.to_csv(\"df_BB.csv\")\n",
    "    df_OX.to_csv(\"df_OX.csv\")\n",
    "    df_PS.to_csv(\"df_PS.csv\")\n",
    "    df_FL.to_csv(\"df_FL.csv\")\n",
    "    df_FL_soccer.to_csv(\"df_FL_soccer.csv\")\n",
    "    df_PS_soccer.to_csv(\"df_PS_soccer.csv\")\n",
    "    df_SB_soccer.to_csv(\"df_SB_soccer.csv\")\n",
    "    df_BL.to_csv(\"df_BL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(df):\n",
    "    ###\n",
    "    #Preparing strings for comparison\n",
    "    ###\n",
    "    for column in ['Player_1', 'Player_2']:\n",
    "        df[column] = df[column].str.lower()\n",
    "        df[column] = df[column].str.replace(\"imperial pro gaming\",'ipg') #replacing some names\n",
    "        df[column] = df[column].str.replace(\"imperialpro\",'ipg') #replacing some names\n",
    "        df[column] = df[column].str.replace(\"five\",'5') #replacing some names\n",
    "        df[column] = df[column].str.replace(\"sv ata spor\",'turkgucu') #replacing some names\n",
    "        df[column] = df[column].str.replace(\"munich\",'munchen') #replacing some names\n",
    "        for word in [\"team\", \"esports\", \"gaming\", \"focusme\", \"e-sports\", \"esport\", \"clan\", \"academy\", \"challenger\",\"!\", \"spvgg\", \"05 (n)\", \"kfc\", \"fsv \", \"tus rw \", \"rot-weiss\", \" sc\", \"vfb \", \" ksc\", \"sc \", \"tsv \", \"fc \", \" fc\"]: \n",
    "            df[column] = df[column].str.replace(word,'') #deleting every annoying word\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merging():\n",
    "    #Creating Match column\n",
    "    for df in [df_BL, df_1X, df_BB, df_OX, df_PS, df_FL, df_SB_soccer, df_PS_soccer, df_FL_soccer]:\n",
    "        df[\"Match\"] = df[\"Player_1\"] + \" - \" + df[\"Player_2\"]\n",
    "\n",
    "    #Merge DFs on Match that is in both DFs\n",
    "    df_DB1 = pd.merge(df_1X, df_FL, on = \"Match\", how = \"inner\")\n",
    "    df_DB2 = pd.merge(df_OX, df_FL, on = \"Match\", how = \"inner\")\n",
    "    df_DB3 = pd.merge(df_FL, df_BB, on = \"Match\", how = \"inner\")\n",
    "    df_DB4 = pd.merge(df_FL, df_PS, on = \"Match\", how = \"inner\")\n",
    "    df_DB5 = pd.merge(df_1X, df_BB, on = \"Match\", how = \"inner\")\n",
    "    #df_DB6 = pd.merge(df_PB, df_BB, on = \"Match\", how = \"inner\")\n",
    "    #df_DB7 = pd.merge(df_OX, df_LB, on = \"Match\", how = \"inner\")\n",
    "    #df_DB8 = pd.merge(df_OX, df_PB, on = \"Match\", how = \"inner\")\n",
    "    df_DB9 = pd.merge(df_OX, df_BB, on = \"Match\", how = \"inner\")\n",
    "    df_DB10 = pd.merge(df_OX, df_1X, on = \"Match\", how = \"inner\")\n",
    "    df_DB11 = pd.merge(df_PS, df_1X, on = \"Match\", how = \"inner\")\n",
    "    df_DB12 = pd.merge(df_PS, df_OX, on = \"Match\", how = \"inner\")\n",
    "    #df_DB13 = pd.merge(df_PS, df_LB, on = \"Match\", how = \"inner\")\n",
    "    #df_DB14 = pd.merge(df_PS, df_PB, on = \"Match\", how = \"inner\")\n",
    "    df_DB15 = pd.merge(df_PS, df_BB, on = \"Match\", how = \"inner\")\n",
    "    df_DB17 = pd.merge(df_FL_soccer, df_SB_soccer, on = \"Match\", how = \"inner\")\n",
    "    df_DB16 = pd.merge(df_PS_soccer, df_SB_soccer, on = \"Match\", how = \"inner\")\n",
    "    df_DB18 = pd.merge(df_PS_soccer, df_FL_soccer, on = \"Match\", how = \"inner\")\n",
    "    df_DB19 = pd.merge(df_BL, df_1X, on = \"Match\", how = \"inner\")\n",
    "    df_DB20 = pd.merge(df_BL, df_BB, on = \"Match\", how = \"inner\")\n",
    "    df_DB21 = pd.merge(df_BL, df_FL, on = \"Match\", how = \"inner\")\n",
    "    df_DB22 = pd.merge(df_BL, df_OX, on = \"Match\", how = \"inner\")\n",
    "    df_DB23 = pd.merge(df_BL, df_PS, on = \"Match\", how = \"inner\")\n",
    "    \n",
    "    df_DB24 = pd.merge(df_BL, df_BL, on = \"Match\", how = \"inner\")\n",
    "    df_DB25 = pd.merge(df_PS, df_PS, on = \"Match\", how = \"inner\")\n",
    "    df_DB26 = pd.merge(df_1X, df_1X, on = \"Match\", how = \"inner\")\n",
    "    df_DB27 = pd.merge(df_OX, df_OX, on = \"Match\", how = \"inner\")\n",
    "    df_DB28 = pd.merge(df_BB, df_BB, on = \"Match\", how = \"inner\")\n",
    "    df_DB29 = pd.merge(df_FL, df_FL, on = \"Match\", how = \"inner\")\n",
    "\n",
    "    df_DB = pd.DataFrame(())\n",
    "    #, df_DB16\n",
    "    for i in [df_DB1, df_DB2, df_DB3, df_DB4, df_DB5, df_DB9, df_DB10, df_DB11, df_DB12, df_DB15, df_DB16, df_DB17, df_DB18, df_DB19, df_DB20, df_DB21, df_DB22, df_DB23, df_DB24, df_DB25, df_DB26, df_DB27, df_DB28, df_DB29]:\n",
    "        df_DB = df_DB.append(i)\n",
    "\n",
    "    df_DB = df_DB.reset_index()\n",
    "    df_DB.drop(\"index\", axis=1)\n",
    "\n",
    "    # Finding the biggest odds\n",
    "    df_odds_1 = pd.DataFrame()\n",
    "    df_odds_1[\"Odds_1_x\"] = df_DB[\"Odds_1_x\"].astype(float)\n",
    "    df_odds_1[\"Odds_1_y\"] = df_DB[\"Odds_1_y\"].astype(float)\n",
    "    df_odds_1[\"max_1\"] = df_odds_1.max(axis=1)\n",
    "    df_odds_2 = pd.DataFrame()\n",
    "    df_odds_2[\"Odds_2_x\"] = df_DB[\"Odds_2_x\"].astype(float)\n",
    "    df_odds_2[\"Odds_2_y\"] = df_DB[\"Odds_2_y\"].astype(float)\n",
    "    df_odds_2[\"max_2\"] = df_odds_2.max(axis=1)\n",
    "\n",
    "    # and stiching them to the main DF df_DB\n",
    "    df_DB = df_DB.join(df_odds_1[\"max_1\"]).join(df_odds_2[\"max_2\"])\n",
    "\n",
    "    # Calculating payout for odds and lays and highlighting good matches\n",
    "    df_DB[\"Payout\"] = (1/df_DB[\"max_1\"] + 1/df_DB[\"max_2\"]) * 100\n",
    "    \n",
    "    df_DB.loc[df_DB[\"Lay_1\"] > 0, \"Payout_Lay_1\"] = df_DB[\"max_1\"]/df_DB[\"Lay_1\"] * 100\n",
    "    df_DB.loc[df_DB[\"Lay_2\"] > 0, \"Payout_Lay_2\"] = df_DB[\"max_2\"]/df_DB[\"Lay_2\"] * 100\n",
    "    df_DB[\"Timestamp\"] = datetime.now()\n",
    "    \n",
    "    # Saving new data to csv file\n",
    "    df_DB.to_csv(\"df_DB_temp.csv\", header=False, index=False)\n",
    "    file = open('df_DB.csv', 'a',  encoding=\"utf8\")\n",
    "    new_data = open('df_DB_temp.csv','r', encoding=\"utf8\")\n",
    "    for row in new_data:\n",
    "        file.write(row)\n",
    "    file.close()\n",
    "    new_data.close()\n",
    "    time.sleep(1)\n",
    "    os.remove(\"df_DB_temp.csv\") \n",
    "    \n",
    "    # Drop all odds with no use like with a Payout above 101\n",
    "    df_DB = df_DB.drop(df_DB[df_DB[\"Payout_Lay_1\"].astype(str) == \"nan\"].index & df_DB[df_DB[\"Payout_Lay_2\"].astype(str) == \"nan\"].index & df_DB[df_DB[\"Payout\"].astype(str) == \"nan\"].index)\n",
    "    df_DB = df_DB.drop(df_DB[df_DB[\"Payout\"] > 101].index)\n",
    "    df_DB = df_DB.sort_values(\"Payout\")\n",
    "    df_DB = df_DB.drop(columns=[\"index\", \"Player_1_y\", \"Player_2_y\", \"Player_1_x\", \"Player_2_x\", \"Lay_1_x\", \"Lay_2_x\", \"Lay_1_y\", \"Lay_2_y\"])\n",
    "    \n",
    "    return df_DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight(x):\n",
    "    ###\n",
    "    ### Colours the background in $colour\n",
    "    ###\n",
    "    c1 = 'background-color: palegreen'\n",
    "    c2 = 'background-color: lightcoral'\n",
    "    c0 = '' # if no default colors\n",
    "    mask1 = (x[\"Payout\"] < 100)\n",
    "    mask2 = (x[\"Payout_Lay_1\"] > 100)\n",
    "    mask3 = (x[\"Payout_Lay_2\"] > 100)\n",
    "    df1 = pd.DataFrame(c0, index=x.index, columns=x.columns)\n",
    "    df1.loc[mask1, 'Payout'] = c1\n",
    "    df1.loc[mask2, 'Payout_Lay_1'] = c2\n",
    "    df1.loc[mask3, 'Payout_Lay_2'] = c2\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    ###\n",
    "    ### \n",
    "    ###\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    df_BL = get_data_BL()\n",
    "    \n",
    "    try:\n",
    "        df_FL = get_data_fl()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        df_SB_soccer = get_data_sb_page()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    df_PS_soccer = get_soccer_data_ps()\n",
    "    df_FL_soccer = get_data_fl_soccer()\n",
    "    df_1X = get_data_1x()\n",
    "    df_BB = get_data_bb()\n",
    "    df_OX = get_data_ox()\n",
    "    df_PS = get_data_ps()\n",
    "\n",
    "    print(\"Webscraping done.\")\n",
    "\n",
    "    df_results = pd.DataFrame()\n",
    "    df_BL.name = \"BL\"\n",
    "    df_FL.name = \"FL\"\n",
    "    df_FL_soccer.name = \"FL_soccer\"\n",
    "    df_1X.name = \"1X\"\n",
    "    df_BB.name = \"BB\"\n",
    "    df_OX.name = \"OX\"\n",
    "    df_PS.name = \"PS\"\n",
    "    df_PS_soccer.name = \"PS_soccer\"\n",
    "    df_SB_soccer.name = \"SB_soccer\"\n",
    "\n",
    "    for i in [df_BL, df_1X, df_BB, df_OX, df_PS, df_FL, df_FL_soccer, df_PS_soccer, df_SB_soccer]:\n",
    "        i[\"Book\"] = i.name\n",
    "\n",
    "    save_to_csv()\n",
    "    print(\"Saving done.\")\n",
    "\n",
    "    cleanup(df_BL)\n",
    "    cleanup(df_1X)\n",
    "    cleanup(df_FL)\n",
    "    cleanup(df_FL_soccer)\n",
    "    cleanup(df_BB)\n",
    "    cleanup(df_OX)\n",
    "    cleanup(df_PS)\n",
    "    cleanup(df_PS_soccer)\n",
    "    cleanup(df_SB_soccer)\n",
    "    print(\"Done cleaning\")\n",
    "    print(\"Ran at: \", pd.to_datetime(datetime.now()).round('10s'))\n",
    "    print(\"Time to run: \", (datetime.now() - start_time))\n",
    "\n",
    "    #Sanity check for all DFs:\n",
    "    for i in [df_BL, df_1X,df_BB, df_OX, df_PS, df_FL, df_FL_soccer, df_PS_soccer, df_SB_soccer]:\n",
    "        print(i.name, len(i))\n",
    "\n",
    "    display(HTML(data=\"\"\"<style>div#notebook-container    { width: 95%; }</style>\"\"\"))\n",
    "\n",
    "    df = merging()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding good matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "# Webscraping starts here\n",
    "df_BL = get_data_BL()\n",
    "df_FL = get_data_fl()\n",
    "df_FL_soccer = get_data_fl_soccer()\n",
    "df_SB_soccer = get_data_sb_page()\n",
    "df_PS_soccer = get_soccer_data_ps()\n",
    "df_1X = get_data_1x()\n",
    "df_BB = get_data_bb()\n",
    "df_OX = get_data_ox()\n",
    "df_PS = get_data_ps()\n",
    "\n",
    "print(\"Webscraping done.\")\n",
    "\n",
    "df_results = pd.DataFrame()\n",
    "df_BL.name = \"BL\"\n",
    "df_FL.name = \"FL\"\n",
    "df_FL_soccer.name = \"FL_soccer\"\n",
    "df_1X.name = \"1X\"\n",
    "df_BB.name = \"BB\"\n",
    "df_OX.name = \"OX\"\n",
    "df_PS.name = \"PS\"\n",
    "df_PS_soccer.name = \"PS_soccer\"\n",
    "df_SB_soccer.name = \"SB_soccer\"\n",
    "\n",
    "for i in [df_BL, df_1X, df_BB, df_OX, df_PS, df_FL, df_FL_soccer, df_PS_soccer, df_SB_soccer]:\n",
    "    i[\"Book\"] = i.name\n",
    "\n",
    "save_to_csv()\n",
    "print(\"Saving done.\")\n",
    "\n",
    "# Data cleaning starts here\n",
    "cleanup(df_BL)\n",
    "cleanup(df_1X)\n",
    "cleanup(df_FL)\n",
    "cleanup(df_FL_soccer)\n",
    "cleanup(df_BB)\n",
    "cleanup(df_OX)\n",
    "cleanup(df_PS)\n",
    "cleanup(df_PS_soccer)\n",
    "cleanup(df_SB_soccer)\n",
    "\n",
    "print(\"Done cleaning\")\n",
    "print(\"Ran at: \", pd.to_datetime(datetime.now()).round('10s'))\n",
    "print(\"Run time: \", (datetime.now() - start_time))\n",
    "\n",
    "#Sanity check for all DFs:\n",
    "for i in [df_BL, df_1X,df_BB, df_OX, df_PS, df_FL, df_FL_soccer, df_PS_soccer, df_SB_soccer]:\n",
    "    print(i.name, len(i))\n",
    "\n",
    "# All results are also saved in an external html file \"Arby_dataframe\" which can be viewed with a browser\n",
    "display(HTML(data=\"\"\"<style>div#notebook-container    { width: 95%; }</style>\"\"\"))\n",
    "\n",
    "df = merging()\n",
    "df.to_html(\"Arby_dataframe.html\")\n",
    "df.style.apply(highlight, axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code runs the above scraping process every 5 minutes and saves the results in an html file\n",
    "status = 1\n",
    "\n",
    "while status == 1:\n",
    "    clear_output(wait=True)\n",
    "    df = run()\n",
    "    df.to_html(\"Arby_dataframe.html\")\n",
    "    time.sleep(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search in ALL games in DFs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All DFs get combined so you can search thoroughly within all games\n",
    "df_all = pd.DataFrame() \n",
    "\n",
    "for i in [df_BL, df_1X, df_BB, df_OX, df_PS, df_FL, df_FL_soccer, df_PS_soccer, df_SB_soccer]:\n",
    "    i[\"Book\"] = i.name\n",
    "    df_all = df_all.append(i)\n",
    "\n",
    "# Search for teams or matches\n",
    "#df_all[df_all[\"Player_1\" or \"Player_2\"] == \"dwg kia\"]\n",
    "df_all[df_all[\"Match\"] == \"epic - vindicta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Or filter by Bookie\n",
    "df_all[df_all[\"Book\"] == \"FL_soccer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Selenium Profile in permanent local folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Get FF Temp Profile Path\")\n",
    "driver.get(\"about:support\")\n",
    "box = driver.find_element_by_id(\"profile-dir-box\")\n",
    "ffTempProfilePath = box.text\n",
    "print(\"ffTempProfilePath: \",ffTempProfilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do your stuff\n",
    "\n",
    "# copy ur stuff after use or periodically\n",
    "print(\"safe Profile\")\n",
    "cwd = os.getcwd()\n",
    "pfadffprofile = cwd+\"\\\\\"+\"ffprofile.selenium\"\n",
    "print (\"saving profile \" + ffTempProfilePath + \" to \" + pfadffprofile)\n",
    "os.system(\"xcopy \" + ffTempProfilePath + \" \" + pfadffprofile)\n",
    "print (\"files should be copied :/\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
